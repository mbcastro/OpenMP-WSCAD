\documentclass{SBCbookchapter}

% Codificação
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% Idioma
\usepackage[brazilian]{babel}

% Referências
\usepackage[brazilian]{backref}
\usepackage[alf]{abntex2cite}

% Figuras
\usepackage[dvipsnames]{xcolor}
\usepackage{graphicx}
\usepackage{subcaption}

% Floats
\usepackage{float}

% Matemática
\usepackage{amsmath}

% Anotações
\usepackage{todonotes}

% Texto Aleatório
\usepackage{lipsum}

% Acrônimos
\usepackage{xspace}
\input{acronimos.tex}

% Tabelas
\usepackage{booktabs}

% Fragmento de Código
\usepackage{listings}
\renewcommand{\lstlistingname}{Fragmento de Código}
\lstset{
	belowcaptionskip=1\baselineskip,
	breaklines=true,
	frame=L,
	xleftmargin=\parindent,
	numbers=left,
	stepnumber=1,
	language=C,
	tabsize=4,
	showstringspaces=false,
	basicstyle=\scriptsize\ttfamily,
	otherkeywords={
		pragma,
		omp,
		parallel,
		num\_threads,
		schedule,
		reduction,
		private,
		shared,
		task,
		taskwait,
		single,
		nowait
	},
	keywordstyle=\color{blue},
	commentstyle=\itshape\color{gray},
	identifierstyle=\color{black},
	stringstyle=\color{purple},
}

% Autores e Instituição
\author{%
	Pedro H. Penna e Márcio Castro \\
	\textit{Laboratório de Pesquisa em Sistemas Distribuídos (LaPeSD)} \\
	\textit{Departamento de Informática e Estatística (INE)}           \\
	\textit{Universidade Federal de Santa Catarina (UFSC)}
}

% Título
\title{Desenvolvimento de Aplicações Paralelas\\Eficientes com \openmp}

\begin{document}

\maketitle

\begin{resumo}
	O emprego de arquiteturas paralelas com quantidades massivas de
	núcleos de processamento é uma tendência na área de Computação de
	Alto Desempenho. Nesse cenário, para que seja feito uso pleno e
	eficiente da capacidade de processamento disponível, torna-se
	imprescindível o uso de bibliotecas de programação paralelas, as
	quais oferecem uma abstração para criação e gerenciamento de
	\textit{threads} e processos. Nesse contexto, esse capítulo
	apresenta uma discussão de como desenvolver aplicações paralelas
	eficientes usando o \openmp, uma interface de programação de
	aplicações para arquiteturas paralelas de memória compartilhada que
	é largamente utilizada pela indústria e academia. Para realizar essa
	discussão, os principais mecanismos oferecidos por essa interface de
	programação são apresentados e analisados através de exemplos
	práticos.
\end{resumo}

\section{Introdução}

	% Processadores Multicore
	Durante as três últimas décadas, avanços constantes de pesquisa na
	área de arquitetura de computadores e de tecnologia na fabricação de
	semicondutores possibilitaram que o desempenho de um único
	processador crescesse linearmente a uma taxa anual de $40\%$ a
	$50\%$~\cite{LARUS08}. No entanto, a dissipação de potência no
	\textit{chip}, inerente ao crescente número de transistores, impôs
	um limite físico ao desempenho máximo possível a ser alcançado.
	Para contornar esse problema, grande parte da indústria agora
	investe no projeto de processadores com diversos núcleos de
	processamento (processadores \textit{multicore}).

	% Computação de Alto Desempenho
	De fato, processadores \textit{multicore} já são largamente
	utilizados na Computação de Alto Desempenho~\cite{Asanovic09}.
	Nesse nicho, a tendência atual é o emprego de centenas, ou até mesmo
	milhares desses processadores no projeto de arquiteturas paralelas
	com uma quantidade massiva de núcleos (\textit{cores}). Essa
	tendência é confirmada pelo \textit{website}
	Top500\footnote{\url{www.top500.org}}, que provê um
	\textit{ranking} atualizado	dos 500 supercomputadores mais poderosos
	do mundo, conforme mostrado na Figura~\ref{fig:top500}.
		
	% Motivação
	No entanto, ao passo que uma quantidade massiva de \textit{cores}
	entrega um grande poder computacional a essas arquiteturas
	paralelas, a alta latência de acesso aos demais dispositivos, como
	memórias e unidades de armazenamento, impõe um desafio arquitetural
	no uso pleno de sua capacidade de desempenho. Para que essa barreira
	seja superada, o desenvolvimento de aplicações paralelas eficientes
	consiste em um requisito obrigatório. Esse objetivo pode ser
	alcançado através do uso de bibliotecas de programação paralela, as
	quais oferecem uma abstração para criação, gerenciamento,
	sincronização e comunicação de \textit{threads} e processos.
	
	Nesse contexto, esse capítulo apresenta uma discussão de como desenvolver
	aplicações paralelas eficientes usando o \openmp, uma Interface de
	Programação de Aplicações (\textit{Aplication Programming Interface}
	-- API) largamente utilizada pela indústria e academia. Além de apresentar
	os conceitos básicos e diretivas de compilação oferecidas pelo OpenMP,
	esse documento apresenta também exemplos práticos de aplicações.
	Diferentes resultados de desempenho obtidos através da execução
	das aplicações em uma arquitetura paralela com 4 processadores
	Intel Xeon X7 (6 \textit{cores} cada) são apresentados e discutidos.
	Todos os exemplos discutidos estão disponíveis para download sob
	licença GNU GPL 3 em \url{https://github.com/lapesd/teaching-openmp}.
	
	Este capítulo está organizado da seguinte forma. A
	Seção~\ref{sec:conceitos basicos} apresenta uma introdução ao modelo
	de programação \openmp.  A Seção~\ref{sec:paralelismo dados}
	apresenta e discute as diretivas do \openmp que podem ser utilizadas
	para realizar o paralelismo de dados. Então, a Seção~\ref{section:
	sincronizacao} apresenta os mecanismos de sincronização básicos
	disponíveis no \openmp. A Seção~\ref{sec:paralelismo tarefas}
	apresenta e discute as diretivas do \openmp que podem ser utilizadas
	para realizar o paralelismo de tarefas. Por fim, a Seção~\ref{sec:conclusao}
	apresenta as conclusões finais.

	\begin{figure}[t]
		\centering
		\includegraphics[width=\linewidth]{img/cores-top500.pdf}
		\caption{Número total de \textit{cores} dos supercomputadores
		mais poderosos listados no \textit{ranking} do Top500.}
		\label{fig:top500}
	\end{figure}


\section{Conceitos Básicos}
\label{sec:conceitos basicos}

	Essa seção irá abordar os conceitos básicos de programação paralela
	com \openmp. Primeiramente, será apresentada uma discussão sobre as
	diferentes classes de arquiteturas paralelas, salientando-se aquelas
	em que \openmp poderá ser aplicado. Em seguida, será apresentado o
	modelo base dessa API.  Por fim, será apresentada a diretiva básica
	para a criação de regiões paralelas no código com as formas de
	compartilhamento de dados oferecidas pelo \openmp.

	\subsection{Arquiteturas Alvo}

		De um modo geral, as arquiteturas paralelas podem ser
		classificadas em três grandes classes, de acordo com a
		organização de sua memória principal: (i) sistemas de memória
		distribuída; (ii) sistemas de memória compartilhada; (iii)
		sistemas de memória compartilhada e distribuída.
	
		Em sistemas de memória distribuída, diversos computadores
		independentes, cada um executando seu próprio sistema
		operacional e com acesso exclusivo à sua memória principal, são
		interconectados através de uma rede e se comunicam através da
		troca de mensagens explícitas. Computadores, ou alternativamente
		nós computacionais, de um sistema de memória distribuída podem
		possuir ou não uma mesma configuração computacional. Quando
		os nós possuem uma mesma configuração e são interconectados
		através de uma rede de alto desempenho (baixa latência e alta 
		largura de banda), o sistema é denominado \textit{cluster}. Por outro
		lado, quando os nós possuem configurações heterogêneas e são
		geograficamente distantes o sistema é usualmente referido como
		\textit{grid}. Sistemas de memória distribuída possuem a
		vantagem de ser altamente escaláveis, no entanto são de difícil
		programação, pois usualmente exigem o reprojeto de algoritmos.
		Uma API bastante utilizada nesses sistemas é a \textit{Message
		Passing Interface} (MPI).
		
		Por outro lado, em sistemas de memória compartilhada, um único
		computador, que é constituído por um ou mais processadores de
		múltiplos \textit{cores} cada, executa apenas uma instância de
		um sistema operacional. A comunicação entre as \textit{threads}
		que executam em \textit{cores} distintos é feita pela memória
		principal, que têm seu acesso globalmente compartilhado.
		Sistemas de memória compartilhada possuem a vantagem de fácil
		programação, no entanto não oferecem alta escalabilidade, como
		sistemas de memória distribuída.
            
		\begin{figure}[t]
			\captionsetup[subfigure]{justification=centering}
			\centering
				\begin{subfigure}{0.45\linewidth}
					\includegraphics[width=\linewidth]{img/uma}
					\caption{UMA.}
					\label{subfigure: uma}
				\end{subfigure}
				\quad\quad
				\begin{subfigure}{0.45\linewidth}
					\includegraphics[width=\linewidth]{img/numa}
					\caption{NUMA.}
					\label{subfigure: numa}
				\end{subfigure}
			\caption{Exemplos de arquiteturas de memória compartilhada.}
			\label{fig:uma-numa}
		\end{figure}

		Sistemas de memória compartilhada, por sua vez, podem ser
		classificados em duas classes, quanto ao tempo de acesso à
		memória principal compartilhada. Uma visão geral dessas duas
		classes é apresentada na Figura~\ref{fig:uma-numa}. Em
		arquiteturas do tipo \textit{Uniform Memory Access} (UMA), o
		tempo de acesso de um \textit{core} à memória é constante
		(Figure~\ref{subfigure: uma}). Por outro lado, em arquiteturas
		do tipo \textit{Non-Uniform Memory Access} (NUMA), esse tempo
		varia em função da organização da memória em
		bancos~(Figura~\ref{subfigure: numa}). Bancos de memória
		locais (próximos ao \textit{core}) possuem uma latência de
		memória menor que bancos de memória remotos (distantes do
		\textit{core}).  Nesse sentido, o tempo de acesso à memória
		principal pode ser pequeno (acesso a um banco de memória
		local) ou grande (acesso a um banco de memória remoto),
		dependendo da distância entre o processador ou \textit{core}
		e o banco de memória que esse processador ou \textit{core}
		está acessando.  Arquiteturas do tipo NUMA possuem maior
		potencial de escalabilidade que arquiteturas UMA. Todavia,
		esses sistemas são mais complexos, caros e de difícil
		programação devido a questões relacionada à afinidade
		de memória entre \textit{threads}~\cite{Pousa-SBAC-2009}.

		Por fim, os sistemas de memória compartilhada e distribuída
		apresentam uma organização mista da memória (compartilhada e
		distribuída) em um único sistema. Por exemplo, nós
		computacionais de \textit{clusters} e \textit{grids} podem
		possuir diversos \textit{cores} cada. Nesse sentido, a memória
		de um computador é compartilhada exclusivamente entre seus
		\textit{cores}. Alternativamente, em processadores
		\textit{manycore} recentes, os \textit{cores} são usualmente
		agrupados em \textit{clusters}. Dentro de um \textit{cluster},
		todos os \textit{cores} compartilham o acesso a uma memória.
		Porém, um \textit{cluster} necessita realizar comunicações via
		rede (usualmente, uma \textit{Network-on-Chip} -- NoC) para que
		possa acessar a memória de outros \textit{clusters}. De fato, sistemas
		de memória compartilhada e distribuída constituem uma tendência na
		Computação de Alto Desempenho, por consistirem em um compromisso
		entre as vantagens dos dois outros tipos de sistemas.

		O \openmp, assunto deste capítulo, consiste numa API para
		sistemas de memória compartilhada e utiliza o conceito de
		\textit{threads} para possibilitar o compartilhamento de dados
		em memória. Por tanto, a API pode ser utilizada tanto para
		arquiteturas UMA quanto para arquiteturas NUMA.
		
	\subsection{Modelo Fork-Join}

		\begin{figure}[b]
			\centering
			\includegraphics[width=0.8\linewidth]{img/fork-join}
			\caption{Visão geral do modelo \textit{Fork-Join}.}\label{fig:fork-join}
		\end{figure}

		% Descrição
		O \openmp segue o modelo \textit{Fork-Join} de execução paralela, que é
		ilustrado na Figura~\ref{fig:fork-join}. Nesse modelo, o
		programa inicia sua execução com uma única \textit{thread},
		denominada \textit{master thread} (Figura~\ref{fig:fork-join}a).
		A \textit{master thread} executa sequencialmente até encontrar
		uma região paralela (Figura~\ref{fig:fork-join}b). Nesse ponto,
		a \textit{master thread} cria um grupo de \textit{threads}
		trabalhadoras, denominadas \textit{worker threads}
		(Figura~\ref{fig:fork-join}c), e cada \textit{worker thread}
		executa então os comandos delimitados pela região paralela. Ao
		concluírem seu trabalho, as \textit{worker threads} sincronizam
		suas atividades e terminam (Figura~\ref{fig:fork-join}d). A
		\textit{master thread} retoma então a execução sequencial do
		programa até que uma nova região paralela seja encontrada,
		momento em que todo esse processo se repete novamente.
		
		% Comentários
		Como observação, é importante deixar claro que a \textit{master
		thread} também executa os comandos na região paralela. Assim, se
		quatro \textit{worker threads} são criadas pela \textit{master
		thread}, um total de cinco \textit{threads} irão executar a
		região paralela.  No entanto, é possível fazer uso de estruturas
		condicionais alinhadas a funções internas do \openmp para
		definir uma execução de comandos diferentes para a
		\textit{master thread}. Esse assunto será abordado mais adiante
		na Seção \ref{section: sincronizacao}. Por fim, vale ressaltar
		que, em implementações modernas do \openmp, a criação das
		estruturas internas para gerência das \textit{threads} em
		regiões paralelas é feita uma única vez, quando a primeira
		região paralela é encontrada. Dessa forma, a sobrecarga imposta
		na aplicação para a criação das \textit{threads} não cresce
		linearmente com o número de regiões paralelas presentes.

	\subsection{Regiões Paralelas e Compartilhamento de Dados}
	\label{subsection: regioes paralelas e compartilhamento de dados}

\begin{lstlisting}[frame=single,float,floatplacement=t,caption=Um exemplo simples com uma região paralela.,label=listing:sayhello] 
void sayhello(int nthreads)
{
	void tids[nthreads]

	/* Cria threads. */
	#pragma omp parallel num_threads(nthreads)
	{
		int tid;

		tid = omp_get_thread_num();
		tids[tid] = tid;
	}

	/* Imprime IDs das threads. */
	for (int i = 0; i < nthreads; i++)
		printf("thread %d: my ID is %d\n", i, tids[i]);
}
\end{lstlisting}

		% Descrição.
		A definição de regiões paralelas no \openmp é feita através da
		diretiva \texttt{omp parallel}, como ilustrado no Fragmento
		de Código~\ref{listing:sayhello}.  Nesse exemplo, assim que a a
		\textit{master thread} atinge a região paralela (linhas 6 a 12),
		ela cria um grupo de \textit{worker threads} para executar os
		comandos especificados. Ao final da região paralela, uma
		barreira implícita força a sincronização das \textit{threads}.
		
		% Controle do Número de Threads
		Por padrão, o número de \textit{threads} no grupo que executará
		uma região paralela será igual ao número de \textit{cores} do
		processador. No entanto, esse fator pode ser controlado através
		da cláusula \texttt{num\_threads()} da região paralela, pela invocação
		da função utilitária \texttt{omp\_set\_num\_threads()}, ou então pela definição da
		variável de ambiente \texttt{OMP\_NUM\_THREADS}. Em uma região
		paralela as \textit{threads} são identificadas por um número
		único entre $0$ e o número total de $\textit{threads} - 1$.
		Esse identificador pode ser recuperado em tempo de execução por
		cada \textit{thread} em uma região paralela através da função utilitária do
		\openmp denominada \texttt{omp\_get\_thread\_num()}.

		% Controle de Escopo de Dados 
		Variáveis declaradas fora da região paralela são compartilhadas
		entre todas as \textit{threads} por padrão. No entanto, a
		cláusula \texttt{private()} pode ser usada para a especificação
		de variáveis locais privadas, e a diretiva \texttt{omp
		threadprivate()} pode ser empregada para a definição de variáveis
		globais privadas. Alternativamente, é possível alterar o
		comportamento padrão para variáveis locais através da cláusula
		\texttt{default()}, definindo todas elas como privadas, e então
		especificar as variáveis locais compartilhadas pela cláusula
		\texttt{shared()}.

		% Programação de Alto Desempenho
		O uso dos mecanismos apresentados anteriormente é fundamental no
		desenvolvimento de aplicações paralelas eficientes. O ajuste do
		número de \textit{threads} em uma região paralela evita o
		desperdício de recursos computacionais e possibilita um ajuste
		da granularidade de tarefas. A identificação de \textit{threads}
		é necessária para a correta coordenação e atribuição de tarefas
		em uma aplicação. Por fim, mecanismos de controle de escopo de
		dados são inerentemente necessários em aplicações paralelas.

\section{Paralelismo de Dados e Diretivas \openmp}
\label{sec:paralelismo dados}

	% Visão Geral
	No \openmp, o paralelismo de dados é explorado através de diretivas
	para paralelização de laços. Nessa seção, o uso dessas diretivas
	para o projeto de aplicações paralelas será discutido.  Primeiro,
	serão apresentadas as diretivas que possibilitam explorar o
	paralelismo de dados. Em seguida, as estratégias de escalonamento
	disponíveis no \openmp serão apresentadas, assim como suas vantagens e
	desvantagens. Por fim, os mecanismos de redução oferecidos pela API
	\openmp serão introduzidos.

	\subsection{Paralelização de Laços}
	\label{subsection: paralelização de lacos}

\begin{lstlisting}[frame=single,float,floatplacement=b,caption=Paralelização da multiplicação de matrizes no \openmp.,label=listing:matrixmult]
struct matrix *matrix_mult(struct matrix *a, struct matrix *b)
{
	struct matrix *c;

	c = matrix_create(a->nrows, b->ncols);

	/* Multiplica matrizes. */
	#pragma omp parallel for private(i, j, k)
	for (int i = 0; i < a->nrows; i++)
	{
		for (int j = 0; j < a->cols; j++)
		{
			for (int k = 0; k < a->ncols; k++)
				MATRIX(c, i, j) += MATRIX(a, i, k)*MATRIX(b, k, j);
		}
	}

	return (c);
}
\end{lstlisting}

		% Visão Geral
		Duas diretivas de paralelização de laços estão disponíveis no
		\openmp: \texttt{omp for} e \texttt{omp parallel for}. A
		primeira diretiva instrui que as iterações do laço seguinte de
		uma região paralela devem ser distribuídas entre as
		\textit{threads} do grupo em questão e, então, executadas em
		paralelo. A segunda diretiva tem o mesmo efeito, porém não
		necessita que o laço esteja dentro de uma região paralela. O
		Fragmento de Código~\ref{listing:matrixmult} ilustra o uso dessa
		última diretiva na paralelização do algoritmo clássico de
		multiplicação de matrizes. Nesse exemplo, um grupo de
		\textit{threads} é criado para executar os comandos do laço da
		linha $9$. Cada \textit{thread} recebe um conjunto distinto de
		iterações e, assim, a computação é efetuada em paralelo. 

		% Discussão de Granularidade
		Observe que nesse exemplo, assim como em diversas outras
		aplicações que também exploram o paralelismo de dados presente
		em laços, a diretiva de compilação também poderia ter sido
		colocada no segundo laço mais externo (linha 11) sem perda de semântica. No
		entanto, as duas abordagens se difeririam quanto à granularidade
		de paralelização. Na primeira abordagem, o grão de trabalho
		entregue a cada \textit{thread} é mais grosso, enquanto na
		segunda abordagem o grão de trabalho é mais fino. O controle de
		granularidade consiste em uma técnica importante no projeto de
		aplicações paralelas eficientes, pois possibilita que a
		localidade temporal e de dados sejam efetivamente exploradas
		além de evitar sobrecargas de sincronização e contornar
		desbalanceamentos de carga durante a computação. Na
		Seção~\ref{subsection: escalonamento de iteracoes} serão
		discutidas as peculiaridades desse assunto em maiores detalhes.

		\begin{figure}[t]
			\captionsetup[subfigure]{justification=centering}
			\centering
				\begin{subfigure}{0.47\linewidth}
					\includegraphics[width=\linewidth]{img/mm}
					\caption{Tempo de execução.}
					\label{figure: time mm}
				\end{subfigure}
				\quad
				\begin{subfigure}{0.47\linewidth}
					\includegraphics[width=\linewidth]{img/mm-cache}
					\caption{Número de faltas de \textit{cache}.}
					\label{figure: cache misses mm}
				\end{subfigure}
				\caption{Resultados das soluções de paralelização para a
				multiplicação de matrizes.}
		\end{figure}
			
		% Análise de Desempenho
		Para ilustrar o impacto no desempenho que cada uma dessas
		abordagens teriam no algoritmo clássico de multiplicação de
		matrizes, experimentos foram conduzidos com tamanho da matriz de
		entrada fixado em $1.680 \times 1.680$ e com o número de \textit{threads} 
		variando de $2$ a $24$. Durante as execuções, foram coletadas estatísticas
		de tempo de execução e número de faltas na \textit{cache} L2.  A análise
		dos resultados de faltas na \textit{cache} L2 (Figura \ref{figure: cache misses
		mm}) revela que a abordagem de paralelização mais grossa conduz
		a um menor número de faltas, em contraste a estratégia de
		paralelização do laço mais interno, para a plataforma e tamanho
		de problema considerados. Essa diferença conduz a um ganho de
		desempenho que é evidenciado nos resultados de tempo de execução
		da aplicação (Figura \ref{figure: time mm}). Além disso, é
		possível constatar que o ganho obtido com mais que $12$
		\textit{threads} não é significantemente superior do que quando
		menos \textit{threads} são usadas, indicando assim que na
		plataforma considerada o tamanho de problema não escala
		linearmente com o número de \textit{threads}.

	\subsection{Escalonamento de Iterações}
	\label{subsection: escalonamento de iteracoes}

		% Aplicações Regulares
		O escalonamento de iterações diz respeito ao modo como iterações
		de um laço paralelo são atribuídas às \textit{threads} de um
		grupo. Por padrão, no \openmp, o escalonamento de iterações é
		feito estaticamente e em blocos (ou \textit{chunks}) de mesmo
		tamanho. Para aplicações que possuem um padrão de computação
		regular, isto é, o tempo de computação para cada iteração do
		laço paralelo é o mesmo, essa estratégia conduz a ganhos de
		desempenhos satisfatórios, pois esse particionamento estático é
		capaz de distribuir a carga de trabalho de forma uniforme entre
		as \textit{threads}. O algoritmo de multiplicação de matrizes
		apresentado na seção anterior exemplifica a classe de algoritmos
		regulares. No caso, a carga de trabalho entregue a cada
		\textit{thread} é constate e proporcional à:
		%
		\begin{equation}
			\text{Carga de Trabalho} = \dfrac{\text{Numéro de Linhas na Matriz}}%
			                                 {\text{Número de Threads}}
		\end{equation}

\begin{lstlisting}[float,floatplacement=b,frame=single, caption=Exemplo de multiplicação de matrizes esparsas.,
label=listing:sparsematrixmult]
struct matrix *sparsematrix_mult(struct matrix *a, struct matrix *b)
{
	struct matrix *c;

	c = matrix_create(a->nrows, b->ncols);

	/* Multiplica matrizes esparsas. */
	#pragma omp parallel for private(i, j, k) schedule(dynamic, 1)
	for (int i = 0; i < a->nrows; i++) {
		for (int j = 0; j < a->ncols; j++) {
			for (int k = 0; k < a->ncols; k++) {
				if (MATRIX(a, i, k) != 0)
					MATRIX(c, i, j) += MATRIX(a, i, k)*MATRIX(b, k, j);
			}
		}
	}

	return (c);
}
\end{lstlisting}
		
		% Aplicações Irregulares
		No entanto, para aplicações que possuem um caráter irregular de
		computação, isto é, o tempo de computação para cada iteração no
		laço paralelo difere; ou então para aplicações regulares que
		possuem afinidade de memória entre diferentes iterações; essa
		estratégia não se mostra eficiente e
		escalável~\cite{Carino2008a}. Então, para atacar esses cenários
		problemáticos, o \openmp disponibiliza a cláusula
		\texttt{schedule()} que possibilita: (i) selecionar a estratégia
		de escalonamento a ser empregada para escalonar as iterações de
		um laço paralelo; e (ii) definir quantas iterações são
		escalonadas por vez a uma única \textit{thread} (tamanho do
		\textit{chunk}).  A estratégia de escalonamento permite
		contornar o problema do desbalanceamento de carga presente em
		aplicações irregulares, enquanto o controle do tamanho de bloco
		de iterações a ser escalonado por vez permite ajustar a
		granularidade de paralelização, possibilitando assim que a
		afinidade de memória existente entre iterações seja explorada.
		O tamanho de bloco de iterações pode ser escolhido
		arbitrariamente, já a estratégia de escalonamento pode ser
		selecionada dentre as seguintes, em uma implementação padrão do
		\openmp:
		%
		\begin{itemize}
			\item \texttt{static}: particiona igualitariamente as
			iterações de um laço paralelo em \textit{chunks} de
			iterações. A atribuição de \textit{chunks} é feita em tempo
			de compilação e nenhuma sobrecarga é adicionada a aplicação.
			Essa estratégia é indicada para aplicações regulares.
			
			\item \texttt{dynamic}: atribui \textit{chunks} de iterações
			às \textit{threads} sob demanda em tempo de execução. Para
			que a atribuição seja feita dinamicamente durante a execução
			da aplicação, uma sobrecarga de gerência é introduzida na
			aplicação. Essa estratégia é indicada para uso em aplicações
			com alto grau de irregularidade.

			\item \texttt{guided}: similar à estratégia
			\texttt{dynamic}, porém o tamanho do \textit{chunk} de
			iterações decresce com o tempo. Essa estratégia também impõe
			uma sobrecarga na execução da aplicação, porém relativamente
			menor que a estratégia \texttt{dynamic}. Essa estratégia é
			indicada para uso em aplicações com baixo e médio graus de
			irregularidade, onde as iterações iniciais do laço paralelo
			podem ser atribuídas em \textit{chunks} maiores, e assim
			alguma sobrecarga de sincronização é evitada durante uma
			porção significativa do tempo total de execução da
			aplicação.
		\end{itemize}
		
		% Análise de Desempenho
		O Fragmento de Código~\ref{listing:sparsematrixmult} ilustra o
		uso desses dois mecanismos na paralelização do algoritmo
		clássico para multiplicação de matrizes esparsas, um típico
		representante de uma aplicação irregular. Nesse exemplo, o
		escalonamento de iterações é feito em blocos (\textit{chunks})
		de tamanho 1 com a política de escalonamento \texttt{dynamic}.
		Uma avaliação quantitativa do desempenho dessa estratégia nesse
		algoritmo particular, frente à estratégia \texttt{static}, é
		apresentada na Figura~\ref{fig:static-dynamic-guided}. Nesses
		experimentos utilizou-se uma matriz de tamanho $1.680 \times 1.680$. 
		O número de \textit{threads} ($n$) foi variado de $2$ a $12$ e o tamanho
		dos \textit{chunks} foi fixado em $1.680/n$ e $1$, para as
		estratégias escalonamento \texttt{static} e \texttt{dynamic},
		respectivamente.  Para essa aplicação, tamanho de problema e
		plataforma, a análise dos resultados revela que a estratégia de
		escalonamento \texttt{dynamic} conduz a um melhor desempenho do
		que a estratégia \texttt{static}, se mostrando assim mais
		escalável.  De fato, esse comportamento é recorrente em
		aplicações irregulares e pode ser utilizado para guiar o projeto
		de soluções paralelas eficientes nesse domínio. No entanto, é
		pertinente observar que a granularidade de \textit{chunks}
		consiste em um fator que impacta diretamente no desempenho
		entregue pela estratégia \texttt{dynamic} e, então, deve ser
		cuidadosamente considerado. 

		% Granularidade de Chunks
		Para enfatizar a influência do tamanho do \textit{chunk} no
		desempenho do escalonador \textit{dynamic}, experimentos foram
		conduzidos com um \textit{kernel} sintético que realiza uma
		computação regular (uma soma de um conjunto grande de números
		inteiros). Diferentes tamanhos de \textit{chunks} foram
		analisados e as estratégias de escalonamento de iterações
		\texttt{static} e \texttt{dynamic} foram consideradas. Os
		resultados obtidos são apresentados na Figura
		\ref{fig:chunk-size} e evidenciam que na estratégia
		\texttt{dynamic}, \textit{chunks} muito pequenos introduzem uma
		expressiva sobrecarga de sincronização na execução, que diminui
		com o aumento do tamanho do \textit{chunk}. Em contraste, a
		estratégia \texttt{static} entrega um desempenho praticamente
		constante à aplicação, independente do tamanho do \textit{chunk}.

		\begin{figure}[t]
			\captionsetup[subfigure]{justification=centering}
			\centering
				\begin{subfigure}{0.47\linewidth}
					\includegraphics[width=\linewidth]{img/smm}
					\caption{Escalabilidade das estratégias \texttt{dynamic} e \texttt{static}.}
					\label{fig:static-dynamic-guided}
				\end{subfigure}
				\quad
				\begin{subfigure}{0.47\linewidth}
					\includegraphics[width=\linewidth]{img/chunk-size}
					\caption{Desempenho para diferentes tamanhos de
					\textit{chunks}.}
					\label{fig:chunk-size}
				\end{subfigure}
			\caption{Desempenho das estratégias \texttt{dynamic} e \texttt{static} obtidos com a versão paralela
			do algoritmo clássico para multiplicação de matrizes esparsas.}
		\end{figure}

	\subsection{Redução de Operações}
	\label{subsection: reducao de operacoes}

		% Visão Geral
		Em aplicações que exploram o paralelismo de dados,
		frequentemente existe uma necessidade de que as \textit{threads}
		combinem seus resultados privados de forma a produzir um
		resultado final para o problema sendo computado. Um suporte
		eficiente a essa operação, denominada redução, é oferecido pelo
		\openmp através da cláusula \texttt{reduction}. O Fragmento de
		Código~\ref{listing:prodescalar} ilustra o uso da operação de
		redução para o cálculo paralelo do produto escalar entre dois
		vetores $a$ e $b$. A definição algébrica do produto escalar entre dois
		vetores ($a \cdot b$) é mostrada na Equação~\ref{eq:scalar-product},
		onde $a=[a_1, a_2, \dots, a_n]$ e $b=[b_1, b_2, \dots, b_n]$:
		
			\begin{equation}
			a \cdot b = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \dots + a_n b_n
			\label{eq:scalar-product}
			\end{equation}		
		
		No Fragmento de Código~\ref{listing:prodescalar}, devido ao uso
		da diretiva \texttt{omp parallel for}, cada \textit{thread} fica responsável por
		calcular o produto de um subconjunto dos elementos dos vetores $a$ e $b$.
		Então, os resultados parciais encontrados por cada \textit{thread} são
		somados, para o cálculo do resultado final.

		% Discussão
		Por padrão, o \openmp oferece suporte  para redução de operações
		aritméticas básicas sobre tipos primitivos da linguagem. Para
		tanto, um cópia privada da variável a ser reduzida é criada em
		cada \textit{thread} e, então, ao final da computação as
		variáveis são combinadas hierarquicamente segundo o operador
		especificado. Para tipos e operadores que o \openmp não oferece
		suporte, é possível fornecer uma implementação eficiente e
		portável fazendo o uso de um laço paralelo, que efetua a
		computação de interesse, seguido por um laço sequencial que
		realiza a redução dos valores parciais computados.

\begin{lstlisting}[float,floatplacement=b,frame=single, caption=Produto escalar.,
label=listing:prodescalar]
int scalar_product(struct vector *a, struct vector *b)
{
	int prod;

	#pragma omp parallel for private(i) schedule (static) reduction(+:prod)
	for (int i = 0; i < a->size; i++)
		prod += VECTOR(a, i) * VECTOR(b, i);
	
	return (prod);
}
\end{lstlisting}


\section{Sincronização}
\label{section: sincronizacao}

	Além de oferecer diretivas úteis para paralelização de porções de
	código, a API \openmp também oferece diretivas para sincronização de
	\textit{worker threads}. Essas diretivas são utilizadas dentro de
	regiões paralelas para: (i) indicar a necessidade de serialização de
	um trecho de código; (ii) garantir exclusão mútua ao acesso a dados
	compartilhados; ou (iii) realizar uma sincronização global entre
	\textit{worker threads}. A seguir, são discutidas as principais
	diretivas de sincronização disponíveis na API \openmp.

		\paragraph{Serialização.} A diretiva \texttt{omp single} permite
		que a execução de um trecho de código dentro de uma região
		paralela seja serializada, ou seja, seja executada por apenas
		uma \textit{worker thread}. O \openmp implementa uma ou diretiva
		similar para serialização de trechos de código denominada
		\texttt{omp master}. A diferença entre as primitivas \texttt{omp
		single} e \texttt{omp master} reside no fato de que, na
		primeira, qualquer \textit{worker thread} pode executar o trecho
		de código serializado, ao passo que, na segunda, somente a
		\textit{master thread} executará o trecho de código serializado.
		
		\paragraph{Exclusão mútua.} Em muitos casos, dados
		compartilhados necessitam modificados dentro de uma região
		paralela por todas as \textit{worker threads}. Nesses trechos de
		código, denominados \textit{seções críticas}, faz-se necessária
		a utilização e um mecanismo de exclusão mútua para evitar que
		duas ou mais \textit{worker threads} tenham acesso
		simultaneamente aos dados compartilhados.  Para esses casos, a
		API \openmp oferece duas diretivas principais. A primeira delas,
		denominada \texttt{omp critical}, implementa um mecanismo de
		exclusão mútua clássico com uso de \textit{locks} similar ao
		\textit{mutex}. A segunda delas, denominada \texttt{omp atomic},
		é semelhante à primeira, porém somente pode ser utilizada para
		proteger operações simples sobre uma variável compartilhada.
		Por exemplo, para uma variável compartilhada \texttt{x}, as
		operações permitidas pela diretiva \texttt{omp atomic} são: (i)
		leitura, \eg \texttt{v = x}; (ii) escrita, \eg \texttt{x =}
		\textit{expr} (onde \textit{expr} é uma expressão aritmética);
		(iii) atualização, \eg \texttt{x++}; ou (iv)
		incremento/decremento, \eg \texttt{v = x++}. A lista completa de
		operações permitidas está disponível na especificação
		\openmp~(??).  É
		importante notar que, para o caso de uma seção crítica contendo
		uma única operação aritmética, ambas as diretivas podem ser
		utilizadas. Porém, nesse caso, a diretiva \texttt{omp atomic}
		oferecerá um melhor desempenho, pois a sua implementação utiliza
		\textit{spin-locks} e instruções de \textit{hardware} para
		garantir a atomicidade das operações.
		
		\paragraph{Sincronização global.} O \openmp também oferece uma
		diretiva que permite realizar uma sincronização global entre as
		\textit{worker threads} na forma de uma \textit{barreira}. Essa
		primitiva, denominada \texttt{omp barrier}, garante que todas as
		\textit{worker threads} somente possam executar as instruções
		posteriores à barreira quando todas as \textit{worker threads}
		tiverem chegado a ela.
	
\section{Paralelismo de Tarefas e Diretivas \openmp}
\label{sec:paralelismo tarefas}

	\begin{figure}[b]
		\centering
		\includegraphics[width=0.6\linewidth]{img/tasks}
		\caption{Paralelismo de tarefas do \openmp: uma ou mais \textit{threads} criam
		tarefas, as quais são inseridas em uma estrutura de dados compartilhada
		que contém tarefas ainda não computadas. \textit{Threads} ociosas em
		uma região paralela retiram tarefas dessa estrutura e as processam uma a
		uma.}
		\label{fig:tasks}
	\end{figure}

	A partir da versão 3.0 foi introduzido o conceito de tarefas
	(\textit{tasks}) no \openmp.  No \openmp a criação de tarefas pode
	ser feita por uma ou mais \textit{threads}. Ao serem criadas, as
	tarefas que ainda não foram computadas são inseridas em uma
	estrutura de dados compartilhada entre as \textit{threads}
	denominada ``saco de tarefas'' (\textit{bag of tasks}). Dentro de
	uma região paralela, ao ficarem ociosas, as \textit{threads} retiram
	tarefas ainda não computadas do saco de tarefas e as processam uma a
	uma.

	\begin{figure}[t]
		\centering
		\includegraphics[width=0.8\linewidth]{img/fibonacci}
		\caption{Grafo de dependência de tarefas gerado pelo algoritmo que calcula a soma da sequência
		de Fibonacci.}\label{fig:fibonacci}
	\end{figure}
	
	A Figura~\ref{fig:tasks} apresenta uma visão geral do modelo de
	tarefas implementado no \openmp. Algumas \textit{threads} somente
	criam tarefas (representadas por flechas unidirecionais com linhas
	tracejadas), outras somente executam tarefas (representadas por
	flechas unidirecionais com linhas pontilhadas), e podem haver
	\textit{threads} que criam e executam tarefas (representadas por
	flechas bidirecionais com linhas contínuas).  A criação e execução
	de tarefas pelas \textit{threads} é feita dentro de uma região
	paralela utilizando a diretiva \texttt{omp task}. Quando uma tarefa
	é criada, a mesma é inserida no saco de tarefas e fica disponível
	para ser computada por qualquer \textit{worker thread} ociosa dentro
	da região paralela.

	Em muitos problemas computacionais paralelizados com o uso de
	tarefas, determinadas tarefas não podem ser computadas pois dependem
	de resultados de outras tarefas ainda não finalizadas. Essas
	dependênncias podem ser vistas na forma de um grafo direcionado de
	dependências, onde cada nó do grafo representa uma tarefa e as
	arestas representam as dependências entre elas. A dependência entre
	tarefas é determinada no \openmp através da diretiva \texttt{omp
	taskwait}. Essa diretiva permite que uma instrução ou um bloco de
	instruções em uma tarefa só seja executado após o término de todas
	as tarefas criadas anteriormente.
	
	Um exemplo clássico de um problema recursivo que pode ser
	paralelizado com o uso de tarefas é a soma da sequência de
	Fibonacci. Matematicamente, os números da sequência de Fibonacci são
	gerados através da seguinte fórmula recursiva:
	
	\begin{align*}
		F_n = F_{n-1} + F_{n-2} \text{, onde } F_1=1 \text{ e } F_2=1
	\end{align*}

	Se considerarmos que para calcular $F_n$ são necessárias duas
	tarefas $F_{n-1}$ e $F_{n-2}$, é evidente que os resultados de
	$F_{n-1}$ e $F_{n-2}$ só poderão ser somados após o término de ambas
	tarefas. Nesse caso, nota-se que $F_n$ depende de $F_{n-1}$ e
	$F_{n-2}$. A Figura~\ref{fig:fibonacci} mostra um exemplo de um
	grafo de dependências de tarefas para o cálculo da soma da sequência
	de Fibonacci usando tarefas. Nesse exemplo, cada tarefa representa
	uma chamada recursiva à $F_{n-1}$ e $F_{n-2}$.

\begin{lstlisting}[float,floatplacement=b,frame=single, caption=Exemplo de uma implementação recursiva simples da soma da sequencia de Fibonacci
usando tarefas., label=listing:fibonacci]
int fibonacci(int n, int stop)
{
	int result;
	
	#pragma omp parallel
  	{
		#pragma omp single nowait
		result = recursive_fibonacci(n, stop);
	}
		
	return (result);
}

long recursive_fibonacci(int n, int stop)
{
	long f1, f2, fn;

	/* Condicao de parada da recursao. */
	if (n == 0 || n == 1) 
		return (n);

	/* Condicao de parada da criacao de tarefas. */
	if (n < stop) 
		return (recursive_fibonacci(n-1, stop) + recursive_fibonacci(n-2, stop));
	else
	{	/* Criacao de tarefas e suas dependencias. */
		#pragma omp task shared(f1)
		f1 = recursive_fibonacci(n-1, stop);

		#pragma omp task shared(f2)
		f2 = recursive_fibonacci(n-2, stop);
		
		#pragma omp taskwait
		fn = f1 + f2;
			
		return (fn);
	}
}
\end{lstlisting}

	O Fragmento de Código~\ref{listing:fibonacci} mostra um exemplo de
	implementação básica para o cálculo da soma da sequência de
	Fibonacci usando tarefas em \openmp. Primeiramente é criada uma
	região paralela com o uso da diretiva \texttt{omp parallel} (linha
	5). Para permitir que somente uma única \textit{worker thread}
	inicie a computação, e consequentemente, a criação das primeiras
	tarefas, utiliza-se a diretiva \texttt{omp single} (linha 7). A
	primeira \textit{worker thread} que ganhar acesso à região
	\texttt{omp single} executa uma chamada à função recursiva
	\texttt{recursive\_fibonacci()}. A cláusula \textit{nowait} permite
	que as demais \textit{worker threads} avancem até a barreira
	implícita ao final da região paralela. Na função
	\texttt{recursive\_fibonacci()}, tarefas correspondentes ao cálculo
	$F_{n-1}$ e $F_{n-2}$ são criadas com o uso da diretiva \texttt{omp
	task} (linhas 27--28 e 30--31) assim como as dependências são
	criadas com a diretiva \texttt{omp taskwait} (linhas 33--34).
	
	Note que o Fragmento de Código~\ref{listing:fibonacci} inclui uma
	condição de parada específica para a criação de tarefas (linha 23),
	evitando-se explicitamente a criação de tarefas quando o valor da
	variável \texttt{n} é inferior a um valor específico
	(\texttt{stop}).  Basicamente, essa condição garante uma carga
	\textit{mínima} para computação de uma tarefa. Em outras palavras, a
	computação será realizada sequencialmente por cada \textit{worker
	thread} quando \texttt{n < stop}. Portanto, a variável \texttt{stop}
	pode ser vista como uma forma de controle de granularidade das
	tarefas da aplicação.  A condição de parada para criação de tarefas
	é muito importante, tendo em vista que a criação e gestão de tarefas
	no \openmp possui um sobrecusto significativo. Logo, é importante
	que as tarefas tenham uma carga grande o suficiente de forma a
	reduzir o sobrecusto anteriormente mencionado. Todavia, tarefas
	contendo cargas extremamente grandes podem gerar desbalanceamentos
	de carga entre as \textit{worker threads}.

	Uma avaliação do impacto da granularidade das tarefas no desempenho
	da aplicação é apresentada na Figura~\ref{fig:grao-tarefas}. Os
	experimentos foram conduzidos em uma máquina UMA com 24 cores ($4
	\times$ Intel Xeon X7 com 6 \textit{cores} cada). Nesses
	experimentos, o número de \textit{threads} foi fixado em 12 e o
	valor da variável \texttt{n} foi fixado em 50.
	
	O resultado mostra um ganho significativo de desempenho quando a
	granularidade das tarefas é aumentada (i.e., valor usado na variável
	\texttt{stop} é aumentada) de $14$ para $26$ (ganho de desempenho de
	$19,8\times$).  Com $\texttt{stop} = 26$ tem-se então o melhor
	compromisso entre sobrecusto da geração/controle de tarefas e
	desbalanceamento de carga entre as \textit{threads}. A partir desse
	ponto, o desempenho da aplicação volta a piorar ligeiramente. Esse
	comportamento é esperando pois um grão muito grande pode gerar
	importantes desbalanceamentos de carga entre as \textit{threads}.
	
		\begin{figure}[t]
			\captionsetup[subfigure]{justification=centering}
			\centering
				\begin{subfigure}{0.47\linewidth}
					\includegraphics[width=\linewidth]{img/fibonacci-task-grain}
					\label{fig:grao-tarefas-tempo}
					\caption{Tempos de execução.}
				\end{subfigure}
				\quad
				\begin{subfigure}{0.47\linewidth}
					\includegraphics[width=\linewidth]{img/fibonacci-speedup}
					\label{fig:grao-tarefas-ganho}
					\caption{Ganhos de desempenho.}
				\end{subfigure}
			\caption{Impacto da granularidade das tarefas no desempenho do cálculo da soma da sequência
				de Fibonacci.}
			\label{fig:grao-tarefas}
		\end{figure}


\section{Conclusão}
\label{sec:conclusao}

	Com a crescente tendência do emprego de arquiteturas paralelas na
	Computação de Alto Desempenho, o desenvolvimento de aplicações
	paralelas eficientes tornou-se um requisito obrigatório. Felizmente,
	essa demanda pode ser suprida através do uso de bibliotecas de
	programação paralela, as quais oferecem uma abstração para a
	criação, gerenciamento, sincronização e comunicação de
	\textit{threads} e processos. Nesse contexto, esse capítulo
	apresenta uma discussão de como desenvolver aplicações paralelas de
	alto desempenho usando o \openmp, uma API largamente utilizada pela
	indústria e academia para arquiteturas paralelas de memória
	compartilhada. 
	
	O \openmp utiliza o conceito de \textit{threads} e recai sobre o
	modelo paralelo de computação \textit{Fork}-\textit{Join}.  Através
	de exemplos práticos, os principais mecanismos oferecidos por essa
	interface de programação foram apresentados e discutidos. O \openmp
	exporta um conjunto de diretivas de compilação e funções
	utilitárias, oferecendo suporte para exploração tanto do paralelismo
	de dados quanto o paralelismo de tarefas. Além disso, mecanismos
	oferecidos para o controle de escalonamento e o ajuste de
	granularidade de tarefas possibilita o desenolvimento de aplicações
	paralelas eficientes.

\bibliography{referencias}

\end{document}
